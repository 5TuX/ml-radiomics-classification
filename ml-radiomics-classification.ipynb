{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07663c35",
   "metadata": {
    "id": "07663c35"
   },
   "source": [
    "# Introductory machine learning experiment with Scikit-Learn\n",
    "\n",
    "A tutorial to showcase a basic machine learning pipeline that you can apply to your own tabular datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f2aab3",
   "metadata": {
    "id": "32f2aab3"
   },
   "source": [
    "## Google colab setup\n",
    "\n",
    "This cell downloads and installs the notebook's requirements in the Google Colab runtime.\n",
    "You should comment or delete it if you are not running the notebook on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769c0c59",
   "metadata": {
    "id": "769c0c59"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Download the repository from GitHub\n",
    "!git clone https://github.com/5TuX/ml-radiomics-classification.git\n",
    "\n",
    "# Set working directory\n",
    "%cd ml-radiomics-classification\n",
    "\n",
    "# Install uv dependency manager\n",
    "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "# Sync dependencies with uv\n",
    "!uv sync\n",
    "\n",
    "# Add src directory to Python path so modules can be imported\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), \"src\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea971a4a",
   "metadata": {
    "id": "ea971a4a"
   },
   "source": [
    "## Python imports and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed3a266",
   "metadata": {
    "id": "2ed3a266"
   },
   "outputs": [],
   "source": [
    "# Python imports\n",
    "import re\n",
    "import platform\n",
    "from itertools import groupby\n",
    "\n",
    "# Data handling imports\n",
    "import polars as pl\n",
    "\n",
    "# Scikit-learn utility imports\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import cross_validate, train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import (\n",
    "    balanced_accuracy_score,\n",
    "    classification_report,\n",
    "    ConfusionMatrixDisplay,\n",
    ")\n",
    "\n",
    "# Scikit-learn model imports\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Other imports\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tabulate import tabulate  # for pretty-printing tables\n",
    "import altair as alt  # for pretty visualizations\n",
    "\n",
    "# Configuration settings\n",
    "settings = {\n",
    "    \"dataset-path\": \"data/Radiomics_binWidth-15_ZScore_NETnNCR_T1CE.csv\",  # our dataset file\n",
    "    \"random_state\": 42,  # set a seed to ensure reproducibility of random procedures\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163caacf",
   "metadata": {
    "id": "163caacf"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Let's imagine we have a reasonably sized **tabular dataset** (e.g. a CSV file) where each line is a sample and each column is a feature.\n",
    "\n",
    "We want to **train** a model to **predict** one of these columns (the target feature) using the other columns (the input features)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fd8fad",
   "metadata": {
    "id": "e3fd8fad"
   },
   "source": [
    "### The dataset\n",
    "\n",
    "In this notebook, we are looking at a version of the [BraTS-2020-Openradiomics](https://openradiomics.org/?page_id=1087) dataset (see [publication](https://doi.org/10.1186/s12880-025-01855-2)) whic contains information of 369 adult patients with brain tumor.\n",
    "\n",
    "<figure style=\"margin: auto; text-align: left; max-width:300px;\">\n",
    "    <img src=\"https://media.springernature.com/full/springer-static/image/art%3A10.1186%2Fs12880-025-01855-2/MediaObjects/12880_2025_1855_Fig1_HTML.png\" style=\"max-width:300px;\">\n",
    "    <figcaption>An example BraTS 2020 image (the FLAIR sequence) and its corresponding segmentation mask. The orange area is AT (active tumor), the green area is ED (peritumoral edematous/invaded tissue), and the gray parts are NETnNCR (Union of necrotic and non-enhancing tumor)</figcaption>\n",
    "</figure>\n",
    "\n",
    "Each line contains (among other things):\n",
    "- Around 1500 [radiomic](https://en.wikipedia.org/wiki/Radiomics) features computed from tumor segmentations of a patient's MRI scan (computation was done using the [PyRadiomics](https://pyradiomics.readthedocs.io/) library). These features provide shape, color and texture information about the tumor. The original MRI images are *not* included.\n",
    "- The category of brain tumor: \"LGG\" for low-grade glioma (less aggressive tumor) or \"HGG\" for high-grade glioma (more aggressive tumor)\n",
    "- Survival information (in days, provided for 236 patients)\n",
    "\n",
    "**Goal:** we will train a binary classifier to predict the class of the tumor (LGG or HGG) from the radiomics features (we'll ignore the survival information entirely)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e522698c",
   "metadata": {
    "id": "e522698c"
   },
   "source": [
    "### Training machine learning models\n",
    "\n",
    "There are a lot of general approaches to machine learning:\n",
    "- Supervised learning trains models on labelled data (most common)\n",
    "- Unsupervised learning searches patterns in unlabelled data\n",
    "- Self-supervised learning uses labels automatically generated from the data itself (e.g., contrastive learning)\n",
    "- Reinforcement learning learns by interacting in an environment to maximize a reward\n",
    "- Many more learning paradigms are listed on wikipedia: https://en.wikipedia.org/wiki/Machine_learning\n",
    "\n",
    "In this notebook, we'll focus on a **supervised classification task**, which consists in predicting a class label from a set of input variables. *(Another common supervised machine learning task is regression, which tries to predict a continuous value. It won't be addressed here.)*\n",
    "\n",
    "The Python **scikit-learn** library provides lots of tools to experiment with machine learning methods on our dataset. In this tutorial, we will showcase some of these tools that will help us to:\n",
    "- **Split** the dataset in training and test subset (crucial to detect overfitting)\n",
    "- Create a **preprocessing** pipeline for the data (category encoding, variable scaling)\n",
    "- Leverage **cross-validation** to quickly compare a bunch of different models\n",
    "- Do the final **training** and **evaluation** of one selected model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40985ac",
   "metadata": {
    "id": "b40985ac"
   },
   "source": [
    "### Spending time with the data\n",
    "\n",
    "Playing around with AI models is only a small part of the job. Most of the time is spent with the dataset to make sure what we do has a meaning. This involves:\n",
    "- loading the data\n",
    "- visualizing and exploring the data\n",
    "- looking at feature distributions and correlations\n",
    "- selecting input and target features\n",
    "- handling missing values\n",
    "- possibly creating new features\n",
    "- ...\n",
    "\n",
    "In this tutorial, we'll hide most of this part to focus on scikit-learn, but keep in mind that in a real-world scenario your dataset will never be ready to use for training out-of-the box.\n",
    "\n",
    "A widely used Python library for handling tabular data is Pandas. In this tutorial however, we prefered to briefly showcase a more recent library named [Polars](https://github.com/pola-rs/polars). Notably, Polars prefers to use [Altair](https://github.com/vega/altair) over Matplotlib for interactive plotting and visualization, which we will also use to gain some visual insights. Polars provides tools to convert to Pandas format, but scikit-learn is compatible with both libraries anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a990b71",
   "metadata": {
    "id": "6a990b71"
   },
   "source": [
    "## Phase 1 - Preparing the data\n",
    "\n",
    "Here we decide what columns to keep, how to handle missing values, and we put a test set aside (20% of the data) for evaluating our final model. All further data pre-processing and model training will be done on the rest of the data without ever looking at the test set.\n",
    "\n",
    "To do all this, we'll use the Polars library (Pandas is also a fine choice)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155778fa",
   "metadata": {
    "id": "155778fa"
   },
   "source": [
    "### Loading the data\n",
    "\n",
    "Let's load the dataset from the CSV file and look at a few raw examples to get an idea of the data structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20253ca4",
   "metadata": {
    "id": "20253ca4"
   },
   "outputs": [],
   "source": [
    "df = pl.read_csv(settings[\"dataset-path\"])\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f872b20a",
   "metadata": {
    "id": "f872b20a"
   },
   "source": [
    "### Looking at some basic statistics\n",
    "\n",
    "The `describe()` method computes basic information about the dataset:\n",
    "\n",
    "- Variable means, standard deviations etc.\n",
    "- Number undefined values in each column:\n",
    "    - Age and survival days are available for only 133 out of 369 patients\n",
    "    - One row seems to have all radiomic features missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6551cf5d",
   "metadata": {
    "id": "6551cf5d"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f8e771",
   "metadata": {
    "id": "f5f8e771"
   },
   "source": [
    "### Making sense of the columns structure\n",
    "\n",
    "There is a huge number of columns in this dataset. Let's quickly explore the structure of column names. In the Pyradiomics documentation, we can learn more about all [radiomics features](https://pyradiomics.readthedocs.io/en/latest/features.html) and the different [image filters](https://pyradiomics.readthedocs.io/en/latest/customization.html#image-types) that can be applied before computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5c22f8",
   "metadata": {
    "id": "1b5c22f8"
   },
   "outputs": [],
   "source": [
    "print(f\"{len(df.columns)} columns in total.\\n\")\n",
    "\n",
    "\n",
    "def key(colname):\n",
    "    return colname.split(\"_\")[0]\n",
    "\n",
    "\n",
    "for column_group, columns in groupby(df.columns, key):\n",
    "    print(column_group)\n",
    "    columns = list(columns)\n",
    "    if len(columns) == 1:\n",
    "        print(f\"\\t-> {columns}\")\n",
    "        continue\n",
    "    columns = [colname[len(column_group) + 1 :] for colname in columns]\n",
    "    for column_subgroup, subcolumns in groupby(columns, key):\n",
    "        maxlen, suffix = 3, \"\"\n",
    "        subcolumns = [colname[len(column_subgroup) + 1 :] for colname in subcolumns]\n",
    "        if (lensubcols := len(subcolumns)) > maxlen:\n",
    "            subcolumns = subcolumns[:maxlen]\n",
    "            suffix = f\"... ({lensubcols} total)\"\n",
    "        print(f\"\\t{column_subgroup} -> {subcolumns} {suffix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5b1222",
   "metadata": {
    "id": "fe5b1222"
   },
   "source": [
    "### Selecting columns to keep\n",
    "\n",
    "We can either specify columns to drop or to keep.\n",
    "In this case, there are 1720 columns, so it's easier to specify which ones to keep.\n",
    "\n",
    "- We set the \"Group\" feature as our target variable\n",
    "- We use all original radiomics (columns with name starting with \"original_\") as input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3e7575",
   "metadata": {
    "id": "2e3e7575"
   },
   "outputs": [],
   "source": [
    "# Target variables:\n",
    "# - ignore Survival_days (too many missing values)\n",
    "# - ignore Group_label (redundant with Group)\n",
    "targets = [\"Group\"]\n",
    "\n",
    "# Clinical inputs:\n",
    "# - ignore Age (many missing values)\n",
    "# - ignore Extent_of_Resection (many missing values)\n",
    "inputs_clinical = []\n",
    "\n",
    "# Radiomic inputs:\n",
    "# - ignore all derived features (columns not starting with \"original_\")\n",
    "inputs_radiomics = [col for col in df.columns if col.startswith(\"original_\")]\n",
    "df = df.select(targets + inputs_clinical + inputs_radiomics).rename(\n",
    "    {col: col[9:] for col in inputs_radiomics}\n",
    ")\n",
    "\n",
    "# Also ignored:\n",
    "# - Patient_ID\n",
    "# - binWidth, Normalization, Subregion, Sequence (describes the data was used to compute the radiomics)\n",
    "# - all columns starting with \"diagnostics_\" (describes the radiomic computation process)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8a2296",
   "metadata": {
    "id": "1a8a2296"
   },
   "source": [
    "### Handling missing values\n",
    "\n",
    "Generally, there are different strategies to handle missing data:\n",
    "- Dropping all rows with missing values (like we do here since there is only one)\n",
    "- Dropping columns with missing values (after checking there is almost no data in them)\n",
    "- Filling missing values, e.g. with the median (imputation)\n",
    "\n",
    "In this case we have seen that there is only one row with missing data, so we'll just get rid of it.\n",
    "\n",
    "Then we'll have a final look at our prepared data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58638f29",
   "metadata": {
    "id": "58638f29"
   },
   "outputs": [],
   "source": [
    "df = df.drop_nulls()\n",
    "print(\"Dataset shape after dropping nulls:\", df.shape)\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f258c5",
   "metadata": {
    "id": "b7f258c5"
   },
   "source": [
    "### Creating a test set\n",
    "\n",
    "It is important to set part of the data aside to make sure our final model performs well on unseen data.\n",
    "\n",
    "- Here we use a stratified split to make ensure that training and test sets have the same class distribution.\n",
    "- We also specify a random seed to ensure reproducibility of the sample shuffling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725aa59b",
   "metadata": {
    "id": "725aa59b"
   },
   "outputs": [],
   "source": [
    "training_data, test_data = train_test_split(\n",
    "    df,\n",
    "    test_size=0.20,\n",
    "    stratify=df[\"Group\"],\n",
    "    random_state=settings[\"random_state\"],\n",
    ")\n",
    "\n",
    "\n",
    "def check_class_proportions(df: pl.DataFrame):\n",
    "    summary = (\n",
    "        df.group_by(\"Group\")\n",
    "        .agg(pl.len().alias(\"Count\"))\n",
    "        .with_columns((pl.col(\"Count\") / pl.col(\"Count\").sum()).alias(\"Proportion\"))\n",
    "        .sort(\"Count\", descending=True)\n",
    "    )\n",
    "    with pl.Config(set_tbl_hide_column_data_types=True, tbl_hide_dataframe_shape=True):\n",
    "        print(summary)\n",
    "\n",
    "\n",
    "print(f\"\\nTraining dataset ({len(training_data)} samples):\")\n",
    "check_class_proportions(training_data)\n",
    "\n",
    "print(f\"\\nTest dataset: ({len(test_data)} samples)\")\n",
    "check_class_proportions(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe434729",
   "metadata": {
    "id": "fe434729"
   },
   "source": [
    "## Phase 2 - Pre-processing the training data\n",
    "\n",
    "The data needs to be pre-preocessed before feeding it to machine learning algorithms:\n",
    "\n",
    "Scikit-learn provides [transformer](https://scikit-learn.org/stable/data_transforms.html) objects to customize your pipelines (unrelated to the famous deep learning transformer architecture). To learn more about the thinking behind this, you can have a look at the library's [design principles](https://doi.org/10.48550/arXiv.1309.0238).\n",
    "\n",
    "Each transformer has two important methods:\n",
    "- `fit` is used a single time to record what preprocessing needs to be done\n",
    "- `transform` can be used any number of times in downstream tasks to apply the pre-processing to new inputs.\n",
    "\n",
    "We'll also use the `Pipeline` and `ColumnTransformer` classes to wrap every transformation in a single final pipeline that we can later apply to any input data.\n",
    "\n",
    "With this slightly complicated but powerful set of tools, we can build virtually any pre-processing pipeline.\n",
    "\n",
    "**Warning**: some pre-processing functions depend on data statistics, so it is crucial to use *only training data* at this stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5e71dc",
   "metadata": {
    "id": "7a5e71dc"
   },
   "source": [
    "### Separating input and target variables\n",
    "\n",
    "Data preprocessing is typically applied to input features only, so let's start by separating input and target variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ece5fc",
   "metadata": {
    "id": "64ece5fc"
   },
   "outputs": [],
   "source": [
    "training_inputs_df = training_data.drop(\"Group\")\n",
    "training_targets_df = training_data.select(\"Group\")\n",
    "\n",
    "print(f\"{type(training_inputs_df)=}\")\n",
    "print(f\"{type(training_targets_df)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdbdd38",
   "metadata": {
    "id": "ffdbdd38"
   },
   "source": [
    "### Encoding the target variables\n",
    "\n",
    "Each sample has a target class label to predict, either \"HGG\" or \"LGG\".\n",
    "\n",
    "Scikit-learn provides a `LabelEncoder` class that converts label strings to label integer IDs. It can be fitted to the target data and later reused to convert the output of the model (class 0, class 1...) to a human-readable format (\"HGG\", \"LGG\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8b9247",
   "metadata": {
    "id": "da8b9247"
   },
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(training_targets_df.to_numpy().ravel())\n",
    "\n",
    "print(f\"{type(y_train) =}, {y_train.shape = }\\n\")\n",
    "print(f\"{y_train[:10] = }\\n\")\n",
    "\n",
    "for i, class_name in enumerate(label_encoder.classes_):\n",
    "    class_support = sum(y_train == i)\n",
    "    print(f\"Class {i}: {class_name} (support: {class_support})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41818c2e",
   "metadata": {
    "id": "41818c2e"
   },
   "source": [
    "### Pre-processing categorical input features\n",
    "\n",
    "Similar to how we encoded target class labels, we also want to use a proper encoding for **categorical input features**: these are originally string values that initially don't mean anything to AI models. They need to be converted to numbers first.\n",
    "\n",
    "In a lot of cases, categorical variables (\"Blue\", \"Red\", \"Green\") have no order. A practical way of representing them is through **one-hot encoding**: we create a binary variable for each possible category, and set all values to zero except for the correct categories: \"Blue\" becomes (1, 0, 0), \"Red\" becomes (0, 1, 0), \"Green\" becomes (0, 0, 1).\n",
    "\n",
    "Scikit-learn provides a `OneHotEncoder` class that can be fitted to our data to convert categorical string data to binary numbers.\n",
    "\n",
    "In this notebook, there are no categorical input variables, but we still write a code to handle them in case the data changes at some point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babe3c93",
   "metadata": {
    "id": "babe3c93"
   },
   "outputs": [],
   "source": [
    "categorical_features = training_inputs_df.select(pl.col(pl.Utf8)).columns\n",
    "\n",
    "print(f\"Number of categorical features: {len(categorical_features)}\")\n",
    "\n",
    "# Define a Pipeline with a single \"onehot\" step:\n",
    "\n",
    "categorical_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"onehot\", OneHotEncoder()),\n",
    "    ]\n",
    ")\n",
    "categorical_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8ba12c",
   "metadata": {
    "id": "fc8ba12c"
   },
   "source": [
    "### Pre-processing numerical input features\n",
    "\n",
    "Most machine learning models don't perform well on numerical features that have widly different scales (some models work perfectly fine with un-normalized data, like the powerful random forest).\n",
    "\n",
    "It therefore is a good practice to apply **feature scaling** methods such as *min-max scaling* and *standardization* (also known as *Z-Score normalization*). Here, we'll go for the latter.\n",
    "\n",
    "Scikit-learn provides a `StandardScaler` class that remembers the mean and variance of the input feaatures and can later be used later to normalize them. (fit this only on training data!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebac40f1",
   "metadata": {
    "id": "ebac40f1"
   },
   "outputs": [],
   "source": [
    "# Handling numerical input variables (feature scaling):\n",
    "\n",
    "numerical_features = training_inputs_df.select(pl.col(pl.Float64)).columns\n",
    "\n",
    "print(f\"Number of numerical features: {len(numerical_features)}\")\n",
    "\n",
    "numerical_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "numerical_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8196553b",
   "metadata": {
    "id": "8196553b"
   },
   "source": [
    "### Defining a complete pre-processing pipeline\n",
    "\n",
    "Once all necessary transformations are defined and wrapped in their respective `Pipeline` objects, we wrap everything in a `ColumnTransformer` that will apply each transformation to the appropriate subset of features.\n",
    "\n",
    "Once that's done, we can fit this big pipeline to our training data and apply the transformation. The `fit_transform` method is a convenient shortcut to fit the pipeline on the data and apply the transformation in a single line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929f417f",
   "metadata": {
    "id": "929f417f"
   },
   "outputs": [],
   "source": [
    "# Define our final pre-processing pipeline:\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numerical_pipeline, numerical_features),\n",
    "        (\"cat\", categorical_pipeline, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit on training data and run pre-processing\n",
    "\n",
    "X_train = preprocessor.fit_transform(training_inputs_df)\n",
    "\n",
    "print(f\"{type(X_train) = }, {X_train.shape = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962ee4b7",
   "metadata": {
    "id": "962ee4b7"
   },
   "source": [
    "### Final look at the pre-processed training data\n",
    "\n",
    "Let's look at one feature distribution before and after pre-processing and check that the average value is now zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57749e74",
   "metadata": {
    "id": "57749e74"
   },
   "outputs": [],
   "source": [
    "# Convert the preprocessed data back to a DataFrame for easier visualization\n",
    "\n",
    "feature_names = preprocessor.get_feature_names_out().tolist()\n",
    "simple_feature_names = [re.sub(r\"^.*?__\", \"\", name) for name in feature_names]\n",
    "X_train_df = pl.DataFrame(X_train, schema=simple_feature_names)\n",
    "\n",
    "# Look at some preprocessed input features distributions:\n",
    "\n",
    "max_features_to_plot = 1\n",
    "\n",
    "print(\"Before preprocessing:\")\n",
    "for feature in training_inputs_df.columns[:max_features_to_plot]:\n",
    "    training_inputs_df[feature].plot.hist().show()\n",
    "\n",
    "print(\"After preprocessing:\")\n",
    "for feature in X_train_df.columns[:max_features_to_plot]:\n",
    "    X_train_df[feature].plot.hist().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db25099d",
   "metadata": {
    "id": "db25099d"
   },
   "source": [
    "## Phase 3 - Cross-validating a few models\n",
    "\n",
    "Now that our data is correctly pre-processed, we can finally start training some models. Since our dataset is small, we can test multiple models in a reasonable amount of time and we can use cross-validation instead of using a regular train/validation split. What does this mean?\n",
    "\n",
    "In a normal train/validation split, we separate part of the training data for validation to estimate how well the model will generalize to unseen data. This helps detect [overfitting](https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html), where a model performs well on training data but poorly on new data. However, a single split can give misleading results if the partition is unbalanced or too small. Cross-validation addresses this by training and evaluating the model multiple times on different folds of the data, ensuring every sample is used for validation once. This provides a more robust and reliable estimate of the model’s true performance while still guarding against overfitting. At the end, we obtain multiple values for our performance metric and typically look at their average.\n",
    "\n",
    "Once cross-validation is complete and a model is selected, we retrain it on the entire training data (both train and validation). Finally, we evaluate it on the test set that was set aside at the beginning and never used during training or model selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94d8f5a",
   "metadata": {
    "id": "f94d8f5a"
   },
   "source": [
    "#### List classification models that we want to compare\n",
    "\n",
    "Here we instantiate some classification models we'd like to try on our training data. This list was essentially taken from scikit-learn documentation's [classifier comparison tutorial](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html).\n",
    "\n",
    "We set a random seed for reproducibility.\n",
    "\n",
    "In this tutorial, we defined two lists of models, one with default parameters and the other with slightly tweaked settings.\n",
    "\n",
    "**Note:** The [MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier) class is an example of a deep learning model. However scikit-learn's implementation is meant for leightweight use only and doesn't include GPU support. To take things to the next level, you should use a library dedicated to deep learning such as [Pytorch](https://github.com/pytorch/pytorch).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bbf912",
   "metadata": {
    "id": "c5bbf912"
   },
   "outputs": [],
   "source": [
    "rnd_state = 42  # set to None for a different random state each time\n",
    "\n",
    "# First list of models with default parameters\n",
    "models1 = {\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=settings[\"random_state\"]),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=settings[\"random_state\"]),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=settings[\"random_state\"]),\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=settings[\"random_state\"]),\n",
    "    \"Linear SVM\": SVC(random_state=settings[\"random_state\"]),\n",
    "    \"RBF SVM\": SVC(random_state=settings[\"random_state\"]),\n",
    "    \"Neural Net\": MLPClassifier(random_state=settings[\"random_state\"]),\n",
    "    \"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "}\n",
    "\n",
    "# Second list of models with a bit more customization\n",
    "models2 = {\n",
    "    \"Logistic Regression\": LogisticRegression(\n",
    "        random_state=settings[\"random_state\"], C=0.5\n",
    "    ),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(\n",
    "        max_depth=3, random_state=settings[\"random_state\"]\n",
    "    ),\n",
    "    \"Random Forest\": RandomForestClassifier(\n",
    "        n_estimators=50, max_depth=5, random_state=settings[\"random_state\"]\n",
    "    ),\n",
    "    \"AdaBoost\": AdaBoostClassifier(\n",
    "        learning_rate=0.6, random_state=settings[\"random_state\"]\n",
    "    ),\n",
    "    \"Linear SVM\": SVC(kernel=\"linear\", C=0.025, random_state=settings[\"random_state\"]),\n",
    "    \"RBF SVM\": SVC(gamma=\"scale\", C=1, random_state=settings[\"random_state\"]),\n",
    "    \"Neural Net\": MLPClassifier(\n",
    "        alpha=1, max_iter=1000, random_state=settings[\"random_state\"]\n",
    "    ),\n",
    "    \"Nearest Neighbors\": KNeighborsClassifier(n_neighbors=3),\n",
    "    \"Naive Bayes\": GaussianNB(var_smoothing=1e-10),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b91234",
   "metadata": {
    "id": "78b91234"
   },
   "source": [
    "#### Decide on a performance metric for model evaluation\n",
    "\n",
    "There are many performance metrics that we can use to evaluate and compare models. Some of the most commons are: accuracy, class recall, class precision, class f1-score. Scikit-learn's documentation lists a huge variety of other metrics that can be used:: https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "\n",
    "Scikit-learn does support custom performance metric definition, but in this notebook, we're going to keep things simple and use a *balanced accuracy score*, since we don't have the same number of samples in each class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76942510",
   "metadata": {
    "id": "76942510"
   },
   "source": [
    "#### Run cross-validation for each model\n",
    "\n",
    "Here we use scikit-learn's `cross-validate` method that returns training and validation scores for each validation split (5 total). We store the average and standard deviation of each score for later comparison. It is important that we do keep both training and validation scores, as the comparison between both allows us to spot oeverfitting issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ca81cd",
   "metadata": {
    "id": "31ca81cd"
   },
   "outputs": [],
   "source": [
    "def cross_validate_models(models, X_train, y_train):\n",
    "    results = []\n",
    "\n",
    "    scoring = \"balanced_accuracy\"\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\nTrying model: {model_name}\")\n",
    "        cv_results = cross_validate(\n",
    "            model,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            cv=5,\n",
    "            scoring=scoring,\n",
    "            return_train_score=True,\n",
    "        )\n",
    "\n",
    "        train_mean = cv_results[\"train_score\"].mean()\n",
    "        train_std = cv_results[\"train_score\"].std()\n",
    "        val_mean = cv_results[\"test_score\"].mean()\n",
    "        val_std = cv_results[\"test_score\"].std()\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"Model\": model_name,\n",
    "                \"Train Mean\": train_mean,\n",
    "                \"Train Std\": train_std,\n",
    "                \"Val Mean\": val_mean,\n",
    "                \"Val Std\": val_std,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print(f\"Train {scoring}: {train_mean:.4f} ± {train_std:.4f}\")\n",
    "        print(f\"Validation {scoring}: {val_mean:.4f} ± {val_std:.4f}\")\n",
    "        print(f\"Train vs Val difference: {train_mean - val_mean:.4f}\")\n",
    "    return results\n",
    "\n",
    "\n",
    "results1 = cross_validate_models(models1, X_train, y_train)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100 + \"\\n\")\n",
    "results2 = cross_validate_models(models2, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343a76c9",
   "metadata": {
    "id": "343a76c9"
   },
   "source": [
    "#### Look at the results and compare the models\n",
    "\n",
    "We then sort the models by average validation score to see which ones perform better.\n",
    "\n",
    "We notice that some models from the first list have a bigger performance difference (above 0.1) between training and validation data, and sometimes a perfect train score. These are signs of overfitting. Ideally, train and validation score should be both high and close to each other.\n",
    "\n",
    "Results from the second models list with tweaked parameters are better, the difference between train and validation scores is lower.\n",
    "\n",
    "Overall, it seems that AdaBoost, Random Forest and Neural Net are the most promising models in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73b2a42",
   "metadata": {
    "id": "b73b2a42"
   },
   "outputs": [],
   "source": [
    "def list_results_sorted(results):\n",
    "    # Sort by validation mean\n",
    "    results = sorted(results, key=lambda x: x[\"Val Mean\"], reverse=True)\n",
    "\n",
    "    # Prepare table for tabulate (remove Val Mean from display)\n",
    "\n",
    "    table = []\n",
    "    for res in results:\n",
    "        train_val_diff = res[\"Train Mean\"] - res[\"Val Mean\"]\n",
    "        table.append(\n",
    "            [\n",
    "                res[\"Model\"],\n",
    "                f\"{res['Train Mean']:.3f} ± {res['Train Std']:.3f}\",\n",
    "                f\"{res['Val Mean']:.3f} ± {res['Val Std']:.3f}\",\n",
    "                train_val_diff,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        tabulate(\n",
    "            table,\n",
    "            headers=[\"Model\", \"Train Score\", \"Validation Score\", \"Train-val Diff\"],\n",
    "            tablefmt=\"fancy_grid\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"\\nResults for models1:\")\n",
    "list_results_sorted(results1)\n",
    "\n",
    "print(\"\\nResults for models2:\")\n",
    "list_results_sorted(results2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea090fb2",
   "metadata": {
    "id": "ea090fb2"
   },
   "source": [
    "## Phase 4 - Fine-tuning a chosen model\n",
    "\n",
    "Looking at the above model comparison, we choose to carry on with the `AdaBoostClassifier` model.\n",
    "\n",
    "Now we want to find the best possible parameters to get the best performance out of this model.\n",
    "\n",
    "Looking at the documentation, we spot two hyperparameters that we can play with: `learning_rate` and `n_estimators` (learn more in the documentation [link text](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9U2k0oMjwIVU",
   "metadata": {
    "id": "9U2k0oMjwIVU"
   },
   "source": [
    "### Searching for the best set of model hyperparameters\n",
    "\n",
    "Scikit-learn provides tools to automatically train and evaluate models with different hyperparameters. We can use a **grid search**, which tests all parameter combinations, or a **random search**, which samples a subset at random.\n",
    "\n",
    "Here we'll go for a random search, since we want to explore a large space but we don't have the time to try every combination.\n",
    "\n",
    "**Note:** after finishing the search, scikit-learn automatically *refits* a model with the best hyperparameters on the entire training set. This final model can be access through the random search's `_best_estimator` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aff6f1",
   "metadata": {
    "id": "82aff6f1"
   },
   "outputs": [],
   "source": [
    "# Define the hyperparameter space to explore\n",
    "param_space = {\n",
    "    \"learning_rate\": [\n",
    "        0.1,\n",
    "        0.2,\n",
    "        0.3,\n",
    "        0.4,\n",
    "        0.5,\n",
    "        0.6,\n",
    "        0.7,\n",
    "        0.8,\n",
    "        0.9,\n",
    "        1,\n",
    "        1.1,\n",
    "        1.2,\n",
    "        1.3,\n",
    "        1.4,\n",
    "        1.5,\n",
    "    ],\n",
    "    \"n_estimators\": [1, 15, 50, 100, 150, 200, 250, 300],\n",
    "}\n",
    "\n",
    "# You can also define hyperparameter distributions like this:\n",
    "# from scipy.stats import randint, uniform\n",
    "# lr_min, lr_max = 0.1, 1.5\n",
    "# learning_rate_distrib = uniform(loc=lr_min, scale=lr_max-lr_min)\n",
    "# n_estimators_distrib = randint(1, 300)\n",
    "\n",
    "# Define and run the randoms search\n",
    "random_search = RandomizedSearchCV(\n",
    "    AdaBoostClassifier(random_state=settings[\"random_state\"]),\n",
    "    param_distributions=param_space,\n",
    "    n_iter=150,\n",
    "    scoring=\"balanced_accuracy\",\n",
    "    cv=5,\n",
    "    random_state=settings[\"random_state\"],\n",
    "    n_jobs=2,\n",
    "    verbose=2,\n",
    ")\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print search results sorted by performance on validation data\n",
    "cv_res = pl.DataFrame(random_search.cv_results_, strict=False)\n",
    "cv_res = cv_res.sort(\"mean_test_score\", descending=True).select(\n",
    "    [\n",
    "        col\n",
    "        for col in cv_res.columns\n",
    "        if not (col.startswith(\"split\") or col.endswith(\"_time\") or col == \"params\")\n",
    "    ]\n",
    ")\n",
    "print(cv_res)\n",
    "\n",
    "# Retrieve the best model\n",
    "print(\"Best model:\")\n",
    "best_model = random_search.best_estimator_\n",
    "best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pZfFuiyDsP61",
   "metadata": {
    "id": "pZfFuiyDsP61"
   },
   "source": [
    "### Looking for patterns in the hyperparameter space (bonus)\n",
    "\n",
    "Let's create a plot to visualize the relationship between the model's hyperparameters and the performance metric.\n",
    "\n",
    "Here we showcase a use of the [Altair](https://github.com/vega/altair) library that is great for interative visualization: you can zoom on the plot and hover your mouse over dots to see their detailed information.\n",
    "\n",
    "*(Coding nice plots can be daunting. This code snippet was generated entirely using ChatGPT)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63833049",
   "metadata": {
    "id": "63833049"
   },
   "outputs": [],
   "source": [
    "min_score = cv_res[\"mean_test_score\"].min()\n",
    "max_score = cv_res[\"mean_test_score\"].max()\n",
    "score_range = max_score - min_score if max_score != min_score else 1.0\n",
    "\n",
    "chart = (\n",
    "    alt.Chart(cv_res)\n",
    "    .transform_calculate(\n",
    "        # normalized version (0–1)\n",
    "        score_norm=f\"(datum.mean_test_score - {min_score}) / {score_range}\",\n",
    "        # exponential scaling for size (power of 3)\n",
    "        size_exp=f\"pow((datum.mean_test_score - {min_score}) / {score_range}, 3)\",\n",
    "    )\n",
    "    .mark_circle()\n",
    "    .encode(\n",
    "        x=\"param_n_estimators\",\n",
    "        y=\"param_learning_rate\",\n",
    "        color=alt.Color(\n",
    "            \"mean_test_score:Q\",\n",
    "            scale=alt.Scale(\n",
    "                domain=[min_score, max_score],\n",
    "                scheme=\"turbo\",  # bright and high contrast\n",
    "                clamp=True,\n",
    "            ),\n",
    "            legend=alt.Legend(title=\"Mean Test Score\"),\n",
    "        ),\n",
    "        size=alt.Size(\n",
    "            \"size_exp:Q\",\n",
    "            scale=alt.Scale(range=[80, 900]),\n",
    "            legend=None,\n",
    "        ),\n",
    "        tooltip=[\"param_n_estimators\", \"param_learning_rate\", \"mean_test_score\"],\n",
    "    )\n",
    "    .properties(\n",
    "        title=\"AdaBoost Hyperparameter Search Results\",\n",
    "        width=600,\n",
    "        height=400,\n",
    "    )\n",
    "    .interactive()\n",
    ")\n",
    "chart.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zJR93QvYvsCh",
   "metadata": {
    "id": "zJR93QvYvsCh"
   },
   "source": [
    "### Testing the final model on the test set\n",
    "\n",
    "Let's do a final evaluation of our model. For this we'll go back to the test data that we set aside at the very beginning.\n",
    "\n",
    "We won't be surprised if the performance is slightly worse since this data was never seen during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c0021b",
   "metadata": {
    "id": "88c0021b"
   },
   "outputs": [],
   "source": [
    "# Test the best model on the held-out test dataset\n",
    "\n",
    "test_inputs_df = test_data.drop(\"Group\")\n",
    "test_targets_df = test_data.select(\"Group\")\n",
    "X_test = preprocessor.transform(test_inputs_df)\n",
    "y_test = label_encoder.transform(test_targets_df.to_numpy().ravel())\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"Balanced Accuracy on test data:\", balanced_accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test, y_pred, display_labels=label_encoder.classes_\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4gmEEnGBl-aH",
   "metadata": {
    "id": "4gmEEnGBl-aH"
   },
   "source": [
    "### Exporting the final pipeline\n",
    "\n",
    "We need a way to persist our pre-processing functions and trained model for future use without having to retrain. An easy way to achieve this is to use the joblib library. Other methods of [model persistance](https://scikit-learn.org/stable/model_persistence.html#pickle-joblib-and-cloudpickle) are listed in scikit-learn's documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XLqRLplGmFaK",
   "metadata": {
    "id": "XLqRLplGmFaK"
   },
   "outputs": [],
   "source": [
    "# Save pre-processing functions and model for alter use\n",
    "model_bundle = {\n",
    "    \"artifacts\": {\n",
    "        \"label_encoder\": label_encoder,\n",
    "        \"pipeline\": Pipeline(\n",
    "            [\n",
    "                (\"preprocessor\", preprocessor),\n",
    "                (\"model\", best_model),\n",
    "            ]\n",
    "        ),\n",
    "    },\n",
    "    \"metadata\": {\n",
    "        \"python_version\": sys.version,\n",
    "        \"platform\": platform.platform(),\n",
    "        \"sklearn_version\": sklearn.__version__,\n",
    "        \"numpy_version\": np.__version__,\n",
    "    },\n",
    "}\n",
    "joblib.dump(model_bundle, \"model.joblib\")\n",
    "\n",
    "# Load the saved pre-processing and model\n",
    "loaded_model = joblib.load(\"model.joblib\")\n",
    "\n",
    "# Run and evaluate the loaded model on test data\n",
    "test_pred = loaded_model[\"artifacts\"][\"pipeline\"].predict(test_inputs_df)\n",
    "test_pred_labels = loaded_model[\"artifacts\"][\"label_encoder\"].inverse_transform(\n",
    "    test_pred\n",
    ")\n",
    "\n",
    "print(\n",
    "    classification_report(\n",
    "        test_targets_df, test_pred_labels, target_names=label_encoder.classes_\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Balanced Accuracy on test data:\", balanced_accuracy_score(y_test, test_pred), \"\\n\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    tabulate(\n",
    "        [\n",
    "            [\"Prediction\", str(test_pred_labels[5:15])],\n",
    "            [\"Expected\", str(test_targets_df.to_numpy().ravel()[5:15])],\n",
    "        ],\n",
    "        tablefmt=\"fancy_grid\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xz1HshAwaZWd",
   "metadata": {
    "id": "xz1HshAwaZWd"
   },
   "source": [
    "## Going further\n",
    "\n",
    "- This notebook does not cover **feature selection**, which is the process of identifying the most relevant variables for a model. Scikit-learn provides various tools for this purpose. More details are available in the [official documentation](https://scikit-learn.org/stable/modules/feature_selection.html).\n",
    "\n",
    "- There are specialized **automated machine learning** (AutoML) libraries for fast model comparison. You can have a look at [pycaret](https://github.com/pycaret/pycaret), [lazypredict](https://github.com/shankarpandala/lazypredict) or [tpot](https://github.com/EpistasisLab/tpot)\n",
    "\n",
    "- For more powerful model **hyperparameter fine-tuning**, you can have a look at a dedicated framework like [optuna](https://github.com/optuna/optuna)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a60da5f",
   "metadata": {
    "id": "0a60da5f"
   },
   "source": [
    "## References\n",
    "\n",
    "- This notebook is heavily inspired by Aurelien Geron's [End-to-end machine learning project](https://github.com/ageron/handson-ml3/blob/main/02_end_to_end_machine_learning_project.ipynb) notebook from his book [Hand-on machine learning with scikit-learn and Pytorch](https://www.oreilly.com/library/view/hands-on-machine-learning/9798341607972/). In this tutorial we're looking at a classification task, but his example focuses on regression. Feel free to check it out if you are interested.\n",
    "- Andrej Karpathy provides a nifty [Recipe for Training Neural Networks](https://karpathy.github.io/2019/04/25/recipe/) which gives useful guidelines for any machine learning project and points at common mistakes to avoid.\n",
    "- The CNRS has a freely accessible program called [FIDLE](https://fidle.cnrs.fr), yearly revisited, wich provides a nice entry point for anyone curious about artifical intelligence."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "https://github.com/5TuX/ml-radiomics-classification/blob/main/ml-radiomics-classification.ipynb",
     "timestamp": 1762337368257
    }
   ]
  },
  "kernelspec": {
   "display_name": "ml-radiomics-classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
